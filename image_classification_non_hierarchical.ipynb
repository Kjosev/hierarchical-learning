{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjosev\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up options\n",
    "\n",
    "options = {}\n",
    "options['margin'] = 50000\n",
    "options['lrate'] = 0.05\n",
    "options['dim'] = 250\n",
    "options['epochs'] = 50\n",
    "options['batch_size'] = 256\n",
    "options['dataset'] = 'awa2'\n",
    "options['num_base_classes'] = 50\n",
    "options['abs'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing the seed\n",
    "np.random.seed(12345)\n",
    "tf.set_random_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base and parent classes: 106\n",
      "Number of hypernym pairs: 1332\n"
     ]
    }
   ],
   "source": [
    "# HYPERNYM PREDICTION DATA\n",
    "hypernyms, name2index = load_hypernym_dataset(options['dataset'], \n",
    "                                              options['num_base_classes'])\n",
    "\n",
    "options['num_all_classes'] = len(name2index)\n",
    "\n",
    "print('Number of base and parent classes:', len(name2index))\n",
    "print('Number of hypernym pairs:', len(hypernyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (37322, 4096)\n",
      "Train size: 29857\n",
      "Validation size: 3733\n",
      "Test size: 3732\n"
     ]
    }
   ],
   "source": [
    "# IMAGE CLASSIFICATION DATA\n",
    "X, labels = load_data()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, labels)\n",
    "\n",
    "print('Dataset size:', X.shape)\n",
    "print('Train size:', len(X_train))\n",
    "print('Validation size:', len(X_val))\n",
    "print('Test size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our Adam optimizer \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=options['lrate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE CLASSIFICATION MODEL\n",
    "im = tf.placeholder(tf.float64, shape=[None, X.shape[1]])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "classification_model = get_classification_model(im, y, options)\n",
    "\n",
    "cls_acc, _, cls_loss, cls_pred = classification_model\n",
    "\n",
    "flat_hit_pred = get_prediction(im, 10, options)\n",
    "\n",
    "cls_train_op = optimizer.minimize(cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our tf session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Steps: 00010 Accuracy: 0.078125\n",
      "Steps: 00020 Accuracy: 0.082031\n",
      "Steps: 00030 Accuracy: 0.074219\n",
      "Steps: 00040 Accuracy: 0.050781\n",
      "Steps: 00050 Accuracy: 0.042969\n",
      "Steps: 00060 Accuracy: 0.058594\n",
      "Steps: 00070 Accuracy: 0.054688\n",
      "Steps: 00080 Accuracy: 0.054688\n",
      "Steps: 00090 Accuracy: 0.035156\n",
      "Steps: 00100 Accuracy: 0.031250\n",
      "Steps: 00110 Accuracy: 0.062500\n",
      "\n",
      "Train Accuracy: 0.054527 Loss: 588294731.382006\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.044736 \n",
      "\n",
      "Epoch: 2\n",
      "Steps: 00120 Accuracy: 0.046875\n",
      "Steps: 00130 Accuracy: 0.046875\n",
      "Steps: 00140 Accuracy: 0.035156\n",
      "Steps: 00150 Accuracy: 0.046875\n",
      "Steps: 00160 Accuracy: 0.035156\n",
      "Steps: 00170 Accuracy: 0.089844\n",
      "Steps: 00180 Accuracy: 0.121094\n",
      "Steps: 00190 Accuracy: 0.093750\n",
      "Steps: 00200 Accuracy: 0.097656\n",
      "Steps: 00210 Accuracy: 0.097656\n",
      "Steps: 00220 Accuracy: 0.105469\n",
      "Steps: 00230 Accuracy: 0.078125\n",
      "\n",
      "Train Accuracy: 0.073919 Loss: 445923425.892463\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.110367 \n",
      "\n",
      "Epoch: 3\n",
      "Steps: 00240 Accuracy: 0.113281\n",
      "Steps: 00250 Accuracy: 0.101562\n",
      "Steps: 00260 Accuracy: 0.113281\n",
      "Steps: 00270 Accuracy: 0.140625\n",
      "Steps: 00280 Accuracy: 0.113281\n",
      "Steps: 00290 Accuracy: 0.101562\n",
      "Steps: 00300 Accuracy: 0.105469\n",
      "Steps: 00310 Accuracy: 0.128906\n",
      "Steps: 00320 Accuracy: 0.097656\n",
      "Steps: 00330 Accuracy: 0.089844\n",
      "Steps: 00340 Accuracy: 0.101562\n",
      "Steps: 00350 Accuracy: 0.105469\n",
      "\n",
      "Train Accuracy: 0.112135 Loss: 329439514.251202\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.134208 \n",
      "\n",
      "Epoch: 4\n",
      "Steps: 00360 Accuracy: 0.144531\n",
      "Steps: 00370 Accuracy: 0.148438\n",
      "Steps: 00380 Accuracy: 0.191406\n",
      "Steps: 00390 Accuracy: 0.265625\n",
      "Steps: 00400 Accuracy: 0.195312\n",
      "Steps: 00410 Accuracy: 0.265625\n",
      "Steps: 00420 Accuracy: 0.296875\n",
      "Steps: 00430 Accuracy: 0.347656\n",
      "Steps: 00440 Accuracy: 0.414062\n",
      "Steps: 00450 Accuracy: 0.519531\n",
      "Steps: 00460 Accuracy: 0.550781\n",
      "\n",
      "Train Accuracy: 0.320427 Loss: 196378775.910245\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.567104 \n",
      "\n",
      "Epoch: 5\n",
      "Steps: 00470 Accuracy: 0.632812\n",
      "Steps: 00480 Accuracy: 0.656250\n",
      "Steps: 00490 Accuracy: 0.660156\n",
      "Steps: 00500 Accuracy: 0.734375\n",
      "Steps: 00510 Accuracy: 0.640625\n",
      "Steps: 00520 Accuracy: 0.644531\n",
      "Steps: 00530 Accuracy: 0.691406\n",
      "Steps: 00540 Accuracy: 0.648438\n",
      "Steps: 00550 Accuracy: 0.652344\n",
      "Steps: 00560 Accuracy: 0.746094\n",
      "Steps: 00570 Accuracy: 0.714844\n",
      "Steps: 00580 Accuracy: 0.718750\n",
      "\n",
      "Train Accuracy: 0.683960 Loss: 106133213.904136\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.732387 \n",
      "\n",
      "Epoch: 6\n",
      "Steps: 00590 Accuracy: 0.625000\n",
      "Steps: 00600 Accuracy: 0.734375\n",
      "Steps: 00610 Accuracy: 0.769531\n",
      "Steps: 00620 Accuracy: 0.808594\n",
      "Steps: 00630 Accuracy: 0.730469\n",
      "Steps: 00640 Accuracy: 0.757812\n",
      "Steps: 00650 Accuracy: 0.769531\n",
      "Steps: 00660 Accuracy: 0.746094\n",
      "Steps: 00670 Accuracy: 0.800781\n",
      "Steps: 00680 Accuracy: 0.753906\n",
      "Steps: 00690 Accuracy: 0.792969\n",
      "Steps: 00700 Accuracy: 0.785156\n",
      "\n",
      "Train Accuracy: 0.762300 Loss: 66535463.561521\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.781677 \n",
      "\n",
      "Epoch: 7\n",
      "Steps: 00710 Accuracy: 0.726562\n",
      "Steps: 00720 Accuracy: 0.746094\n",
      "Steps: 00730 Accuracy: 0.777344\n",
      "Steps: 00740 Accuracy: 0.785156\n",
      "Steps: 00750 Accuracy: 0.789062\n",
      "Steps: 00760 Accuracy: 0.808594\n",
      "Steps: 00770 Accuracy: 0.808594\n",
      "Steps: 00780 Accuracy: 0.792969\n",
      "Steps: 00790 Accuracy: 0.820312\n",
      "Steps: 00800 Accuracy: 0.773438\n",
      "Steps: 00810 Accuracy: 0.769531\n",
      "\n",
      "Train Accuracy: 0.794621 Loss: 46851715.603020\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.802304 \n",
      "\n",
      "Epoch: 8\n",
      "Steps: 00820 Accuracy: 0.734375\n",
      "Steps: 00830 Accuracy: 0.792969\n",
      "Steps: 00840 Accuracy: 0.832031\n",
      "Steps: 00850 Accuracy: 0.804688\n",
      "Steps: 00860 Accuracy: 0.796875\n",
      "Steps: 00870 Accuracy: 0.835938\n",
      "Steps: 00880 Accuracy: 0.828125\n",
      "Steps: 00890 Accuracy: 0.777344\n",
      "Steps: 00900 Accuracy: 0.839844\n",
      "Steps: 00910 Accuracy: 0.855469\n",
      "Steps: 00920 Accuracy: 0.839844\n",
      "Steps: 00930 Accuracy: 0.832031\n",
      "\n",
      "Train Accuracy: 0.812841 Loss: 35911462.042962\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.817037 \n",
      "\n",
      "Epoch: 9\n",
      "Steps: 00940 Accuracy: 0.824219\n",
      "Steps: 00950 Accuracy: 0.843750\n",
      "Steps: 00960 Accuracy: 0.812500\n",
      "Steps: 00970 Accuracy: 0.792969\n",
      "Steps: 00980 Accuracy: 0.835938\n",
      "Steps: 00990 Accuracy: 0.816406\n",
      "Steps: 01000 Accuracy: 0.808594\n",
      "Steps: 01010 Accuracy: 0.828125\n",
      "Steps: 01020 Accuracy: 0.863281\n",
      "Steps: 01030 Accuracy: 0.859375\n",
      "Steps: 01040 Accuracy: 0.851562\n",
      "Steps: 01050 Accuracy: 0.882812\n",
      "\n",
      "Train Accuracy: 0.826473 Loss: 28977669.268954\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.827217 \n",
      "\n",
      "Epoch: 10\n",
      "Steps: 01060 Accuracy: 0.804688\n",
      "Steps: 01070 Accuracy: 0.824219\n",
      "Steps: 01080 Accuracy: 0.847656\n",
      "Steps: 01090 Accuracy: 0.816406\n",
      "Steps: 01100 Accuracy: 0.859375\n",
      "Steps: 01110 Accuracy: 0.871094\n",
      "Steps: 01120 Accuracy: 0.828125\n",
      "Steps: 01130 Accuracy: 0.804688\n",
      "Steps: 01140 Accuracy: 0.812500\n",
      "Steps: 01150 Accuracy: 0.835938\n",
      "Steps: 01160 Accuracy: 0.851562\n",
      "Steps: 01170 Accuracy: 0.857143\n",
      "\n",
      "Train Accuracy: 0.835750 Loss: 24172324.808834\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.841682 \n",
      "\n",
      "Epoch: 11\n",
      "Steps: 01180 Accuracy: 0.859375\n",
      "Steps: 01190 Accuracy: 0.792969\n",
      "Steps: 01200 Accuracy: 0.863281\n",
      "Steps: 01210 Accuracy: 0.835938\n",
      "Steps: 01220 Accuracy: 0.863281\n",
      "Steps: 01230 Accuracy: 0.843750\n",
      "Steps: 01240 Accuracy: 0.886719\n",
      "Steps: 01250 Accuracy: 0.839844\n",
      "Steps: 01260 Accuracy: 0.875000\n",
      "Steps: 01270 Accuracy: 0.859375\n",
      "Steps: 01280 Accuracy: 0.843750\n",
      "\n",
      "Train Accuracy: 0.850688 Loss: 20657076.496506\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.843557 \n",
      "\n",
      "Epoch: 12\n",
      "Steps: 01290 Accuracy: 0.828125\n",
      "Steps: 01300 Accuracy: 0.878906\n",
      "Steps: 01310 Accuracy: 0.855469\n",
      "Steps: 01320 Accuracy: 0.835938\n",
      "Steps: 01330 Accuracy: 0.785156\n",
      "Steps: 01340 Accuracy: 0.832031\n",
      "Steps: 01350 Accuracy: 0.808594\n",
      "Steps: 01360 Accuracy: 0.890625\n",
      "Steps: 01370 Accuracy: 0.824219\n",
      "Steps: 01380 Accuracy: 0.859375\n",
      "Steps: 01390 Accuracy: 0.882812\n",
      "Steps: 01400 Accuracy: 0.843750\n",
      "\n",
      "Train Accuracy: 0.855913 Loss: 17814249.618543\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.847040 \n",
      "\n",
      "Epoch: 13\n",
      "Steps: 01410 Accuracy: 0.855469\n",
      "Steps: 01420 Accuracy: 0.875000\n",
      "Steps: 01430 Accuracy: 0.894531\n",
      "Steps: 01440 Accuracy: 0.886719\n",
      "Steps: 01450 Accuracy: 0.898438\n",
      "Steps: 01460 Accuracy: 0.847656\n",
      "Steps: 01470 Accuracy: 0.882812\n",
      "Steps: 01480 Accuracy: 0.867188\n",
      "Steps: 01490 Accuracy: 0.890625\n",
      "Steps: 01500 Accuracy: 0.941406\n",
      "Steps: 01510 Accuracy: 0.882812\n",
      "Steps: 01520 Accuracy: 0.847656\n",
      "\n",
      "Train Accuracy: 0.868708 Loss: 15649493.515204\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.858023 \n",
      "\n",
      "Epoch: 14\n",
      "Steps: 01530 Accuracy: 0.906250\n",
      "Steps: 01540 Accuracy: 0.921875\n",
      "Steps: 01550 Accuracy: 0.867188\n",
      "Steps: 01560 Accuracy: 0.886719\n",
      "Steps: 01570 Accuracy: 0.906250\n",
      "Steps: 01580 Accuracy: 0.882812\n",
      "Steps: 01590 Accuracy: 0.890625\n",
      "Steps: 01600 Accuracy: 0.910156\n",
      "Steps: 01610 Accuracy: 0.855469\n",
      "Steps: 01620 Accuracy: 0.894531\n",
      "Steps: 01630 Accuracy: 0.859375\n",
      "\n",
      "Train Accuracy: 0.879392 Loss: 13854217.451237\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.864184 \n",
      "\n",
      "Epoch: 15\n",
      "Steps: 01640 Accuracy: 0.882812\n",
      "Steps: 01650 Accuracy: 0.910156\n",
      "Steps: 01660 Accuracy: 0.894531\n",
      "Steps: 01670 Accuracy: 0.929688\n",
      "Steps: 01680 Accuracy: 0.863281\n",
      "Steps: 01690 Accuracy: 0.871094\n",
      "Steps: 01700 Accuracy: 0.878906\n",
      "Steps: 01710 Accuracy: 0.875000\n",
      "Steps: 01720 Accuracy: 0.875000\n",
      "Steps: 01730 Accuracy: 0.882812\n",
      "Steps: 01740 Accuracy: 0.871094\n",
      "Steps: 01750 Accuracy: 0.859375\n",
      "\n",
      "Train Accuracy: 0.883679 Loss: 12464526.711021\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.866595 \n",
      "\n",
      "Epoch: 16\n",
      "Steps: 01760 Accuracy: 0.890625\n",
      "Steps: 01770 Accuracy: 0.863281\n",
      "Steps: 01780 Accuracy: 0.921875\n",
      "Steps: 01790 Accuracy: 0.878906\n",
      "Steps: 01800 Accuracy: 0.886719\n",
      "Steps: 01810 Accuracy: 0.886719\n",
      "Steps: 01820 Accuracy: 0.898438\n",
      "Steps: 01830 Accuracy: 0.937500\n",
      "Steps: 01840 Accuracy: 0.882812\n",
      "Steps: 01850 Accuracy: 0.882812\n",
      "Steps: 01860 Accuracy: 0.878906\n",
      "Steps: 01870 Accuracy: 0.875000\n",
      "\n",
      "Train Accuracy: 0.889909 Loss: 11137009.563011\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.869274 \n",
      "\n",
      "Epoch: 17\n",
      "Steps: 01880 Accuracy: 0.894531\n",
      "Steps: 01890 Accuracy: 0.921875\n",
      "Steps: 01900 Accuracy: 0.898438\n",
      "Steps: 01910 Accuracy: 0.917969\n",
      "Steps: 01920 Accuracy: 0.894531\n",
      "Steps: 01930 Accuracy: 0.910156\n",
      "Steps: 01940 Accuracy: 0.921875\n",
      "Steps: 01950 Accuracy: 0.906250\n",
      "Steps: 01960 Accuracy: 0.886719\n",
      "Steps: 01970 Accuracy: 0.917969\n",
      "Steps: 01980 Accuracy: 0.890625\n",
      "\n",
      "Train Accuracy: 0.893794 Loss: 10044069.477873\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.870346 \n",
      "\n",
      "Epoch: 18\n",
      "Steps: 01990 Accuracy: 0.910156\n",
      "Steps: 02000 Accuracy: 0.921875\n",
      "Steps: 02010 Accuracy: 0.929688\n",
      "Steps: 02020 Accuracy: 0.867188\n",
      "Steps: 02030 Accuracy: 0.894531\n",
      "Steps: 02040 Accuracy: 0.875000\n",
      "Steps: 02050 Accuracy: 0.914062\n",
      "Steps: 02060 Accuracy: 0.886719\n",
      "Steps: 02070 Accuracy: 0.917969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 02080 Accuracy: 0.921875\n",
      "Steps: 02090 Accuracy: 0.886719\n",
      "Steps: 02100 Accuracy: 0.886719\n",
      "\n",
      "Train Accuracy: 0.897846 Loss: 9066075.262786\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.870881 \n",
      "\n",
      "Epoch: 19\n",
      "Steps: 02110 Accuracy: 0.878906\n",
      "Steps: 02120 Accuracy: 0.890625\n",
      "Steps: 02130 Accuracy: 0.894531\n",
      "Steps: 02140 Accuracy: 0.871094\n",
      "Steps: 02150 Accuracy: 0.890625\n",
      "Steps: 02160 Accuracy: 0.910156\n",
      "Steps: 02170 Accuracy: 0.906250\n",
      "Steps: 02180 Accuracy: 0.914062\n",
      "Steps: 02190 Accuracy: 0.890625\n",
      "Steps: 02200 Accuracy: 0.910156\n",
      "Steps: 02210 Accuracy: 0.933594\n",
      "Steps: 02220 Accuracy: 0.878906\n",
      "\n",
      "Train Accuracy: 0.902401 Loss: 8161257.348850\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.870346 \n",
      "\n",
      "Epoch: 20\n",
      "Steps: 02230 Accuracy: 0.878906\n",
      "Steps: 02240 Accuracy: 0.925781\n",
      "Steps: 02250 Accuracy: 0.910156\n",
      "Steps: 02260 Accuracy: 0.929688\n",
      "Steps: 02270 Accuracy: 0.902344\n",
      "Steps: 02280 Accuracy: 0.914062\n",
      "Steps: 02290 Accuracy: 0.929688\n",
      "Steps: 02300 Accuracy: 0.925781\n",
      "Steps: 02310 Accuracy: 0.917969\n",
      "Steps: 02320 Accuracy: 0.917969\n",
      "Steps: 02330 Accuracy: 0.906250\n",
      "Steps: 02340 Accuracy: 0.900621\n",
      "\n",
      "Train Accuracy: 0.907760 Loss: 7436799.960544\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.878918 \n",
      "\n",
      "Epoch: 21\n",
      "Steps: 02350 Accuracy: 0.890625\n",
      "Steps: 02360 Accuracy: 0.906250\n",
      "Steps: 02370 Accuracy: 0.910156\n",
      "Steps: 02380 Accuracy: 0.878906\n",
      "Steps: 02390 Accuracy: 0.914062\n",
      "Steps: 02400 Accuracy: 0.921875\n",
      "Steps: 02410 Accuracy: 0.933594\n",
      "Steps: 02420 Accuracy: 0.949219\n",
      "Steps: 02430 Accuracy: 0.910156\n",
      "Steps: 02440 Accuracy: 0.933594\n",
      "Steps: 02450 Accuracy: 0.945312\n",
      "\n",
      "Train Accuracy: 0.912081 Loss: 6691141.936226\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.877310 \n",
      "\n",
      "Epoch: 22\n",
      "Steps: 02460 Accuracy: 0.917969\n",
      "Steps: 02470 Accuracy: 0.925781\n",
      "Steps: 02480 Accuracy: 0.921875\n",
      "Steps: 02490 Accuracy: 0.898438\n",
      "Steps: 02500 Accuracy: 0.902344\n",
      "Steps: 02510 Accuracy: 0.886719\n",
      "Steps: 02520 Accuracy: 0.910156\n",
      "Steps: 02530 Accuracy: 0.910156\n",
      "Steps: 02540 Accuracy: 0.933594\n",
      "Steps: 02550 Accuracy: 0.886719\n",
      "Steps: 02560 Accuracy: 0.906250\n",
      "Steps: 02570 Accuracy: 0.921875\n",
      "\n",
      "Train Accuracy: 0.915296 Loss: 6121937.759602\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.877310 \n",
      "\n",
      "Epoch: 23\n",
      "Steps: 02580 Accuracy: 0.921875\n",
      "Steps: 02590 Accuracy: 0.917969\n",
      "Steps: 02600 Accuracy: 0.945312\n",
      "Steps: 02610 Accuracy: 0.890625\n",
      "Steps: 02620 Accuracy: 0.902344\n",
      "Steps: 02630 Accuracy: 0.941406\n",
      "Steps: 02640 Accuracy: 0.929688\n",
      "Steps: 02650 Accuracy: 0.898438\n",
      "Steps: 02660 Accuracy: 0.917969\n",
      "Steps: 02670 Accuracy: 0.925781\n",
      "Steps: 02680 Accuracy: 0.933594\n",
      "Steps: 02690 Accuracy: 0.925781\n",
      "\n",
      "Train Accuracy: 0.919181 Loss: 5565563.173842\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.881597 \n",
      "\n",
      "Epoch: 24\n",
      "Steps: 02700 Accuracy: 0.902344\n",
      "Steps: 02710 Accuracy: 0.914062\n",
      "Steps: 02720 Accuracy: 0.925781\n",
      "Steps: 02730 Accuracy: 0.933594\n",
      "Steps: 02740 Accuracy: 0.933594\n",
      "Steps: 02750 Accuracy: 0.921875\n",
      "Steps: 02760 Accuracy: 0.937500\n",
      "Steps: 02770 Accuracy: 0.921875\n",
      "Steps: 02780 Accuracy: 0.933594\n",
      "Steps: 02790 Accuracy: 0.941406\n",
      "Steps: 02800 Accuracy: 0.871094\n",
      "\n",
      "Train Accuracy: 0.923770 Loss: 5111670.097455\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.883472 \n",
      "\n",
      "Epoch: 25\n",
      "Steps: 02810 Accuracy: 0.925781\n",
      "Steps: 02820 Accuracy: 0.945312\n",
      "Steps: 02830 Accuracy: 0.902344\n",
      "Steps: 02840 Accuracy: 0.953125\n",
      "Steps: 02850 Accuracy: 0.921875\n",
      "Steps: 02860 Accuracy: 0.937500\n",
      "Steps: 02870 Accuracy: 0.882812\n",
      "Steps: 02880 Accuracy: 0.902344\n",
      "Steps: 02890 Accuracy: 0.949219\n",
      "Steps: 02900 Accuracy: 0.945312\n",
      "Steps: 02910 Accuracy: 0.914062\n",
      "Steps: 02920 Accuracy: 0.917969\n",
      "\n",
      "Train Accuracy: 0.926282 Loss: 4761086.919252\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.884007 \n",
      "\n",
      "Epoch: 26\n",
      "Steps: 02930 Accuracy: 0.937500\n",
      "Steps: 02940 Accuracy: 0.937500\n",
      "Steps: 02950 Accuracy: 0.957031\n",
      "Steps: 02960 Accuracy: 0.929688\n",
      "Steps: 02970 Accuracy: 0.929688\n",
      "Steps: 02980 Accuracy: 0.949219\n",
      "Steps: 02990 Accuracy: 0.925781\n",
      "Steps: 03000 Accuracy: 0.906250\n",
      "Steps: 03010 Accuracy: 0.902344\n",
      "Steps: 03020 Accuracy: 0.937500\n",
      "Steps: 03030 Accuracy: 0.914062\n",
      "Steps: 03040 Accuracy: 0.949219\n",
      "\n",
      "Train Accuracy: 0.930870 Loss: 4366111.509558\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.884811 \n",
      "\n",
      "Epoch: 27\n",
      "Steps: 03050 Accuracy: 0.917969\n",
      "Steps: 03060 Accuracy: 0.941406\n",
      "Steps: 03070 Accuracy: 0.937500\n",
      "Steps: 03080 Accuracy: 0.933594\n",
      "Steps: 03090 Accuracy: 0.953125\n",
      "Steps: 03100 Accuracy: 0.921875\n",
      "Steps: 03110 Accuracy: 0.925781\n",
      "Steps: 03120 Accuracy: 0.937500\n",
      "Steps: 03130 Accuracy: 0.945312\n",
      "Steps: 03140 Accuracy: 0.929688\n",
      "Steps: 03150 Accuracy: 0.949219\n",
      "\n",
      "Train Accuracy: 0.935057 Loss: 4054795.504957\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.886954 \n",
      "\n",
      "Epoch: 28\n",
      "Steps: 03160 Accuracy: 0.921875\n",
      "Steps: 03170 Accuracy: 0.945312\n",
      "Steps: 03180 Accuracy: 0.953125\n",
      "Steps: 03190 Accuracy: 0.949219\n",
      "Steps: 03200 Accuracy: 0.945312\n",
      "Steps: 03210 Accuracy: 0.937500\n",
      "Steps: 03220 Accuracy: 0.914062\n",
      "Steps: 03230 Accuracy: 0.945312\n",
      "Steps: 03240 Accuracy: 0.933594\n",
      "Steps: 03250 Accuracy: 0.933594\n",
      "Steps: 03260 Accuracy: 0.925781\n",
      "Steps: 03270 Accuracy: 0.945312\n",
      "\n",
      "Train Accuracy: 0.939311 Loss: 3808087.256887\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.889633 \n",
      "\n",
      "Epoch: 29\n",
      "Steps: 03280 Accuracy: 0.953125\n",
      "Steps: 03290 Accuracy: 0.957031\n",
      "Steps: 03300 Accuracy: 0.945312\n",
      "Steps: 03310 Accuracy: 0.937500\n",
      "Steps: 03320 Accuracy: 0.925781\n",
      "Steps: 03330 Accuracy: 0.937500\n",
      "Steps: 03340 Accuracy: 0.953125\n",
      "Steps: 03350 Accuracy: 0.960938\n",
      "Steps: 03360 Accuracy: 0.925781\n",
      "Steps: 03370 Accuracy: 0.929688\n",
      "Steps: 03380 Accuracy: 0.945312\n",
      "Steps: 03390 Accuracy: 0.937500\n",
      "\n",
      "Train Accuracy: 0.942627 Loss: 3529788.172453\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.887222 \n",
      "\n",
      "Epoch: 30\n",
      "Steps: 03400 Accuracy: 0.949219\n",
      "Steps: 03410 Accuracy: 0.925781\n",
      "Steps: 03420 Accuracy: 0.949219\n",
      "Steps: 03430 Accuracy: 0.941406\n",
      "Steps: 03440 Accuracy: 0.964844\n",
      "Steps: 03450 Accuracy: 0.921875\n",
      "Steps: 03460 Accuracy: 0.921875\n",
      "Steps: 03470 Accuracy: 0.941406\n",
      "Steps: 03480 Accuracy: 0.945312\n",
      "Steps: 03490 Accuracy: 0.941406\n",
      "Steps: 03500 Accuracy: 0.957031\n",
      "Steps: 03510 Accuracy: 0.975155\n",
      "\n",
      "Train Accuracy: 0.946445 Loss: 3290626.938314\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.889365 \n",
      "\n",
      "Epoch: 31\n",
      "Steps: 03520 Accuracy: 0.960938\n",
      "Steps: 03530 Accuracy: 0.964844\n",
      "Steps: 03540 Accuracy: 0.957031\n",
      "Steps: 03550 Accuracy: 0.925781\n",
      "Steps: 03560 Accuracy: 0.937500\n",
      "Steps: 03570 Accuracy: 0.914062\n",
      "Steps: 03580 Accuracy: 0.929688\n",
      "Steps: 03590 Accuracy: 0.929688\n",
      "Steps: 03600 Accuracy: 0.988281\n",
      "Steps: 03610 Accuracy: 0.941406\n",
      "Steps: 03620 Accuracy: 0.945312\n",
      "\n",
      "Train Accuracy: 0.948555 Loss: 3084451.558190\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.890437 \n",
      "\n",
      "Epoch: 32\n",
      "Steps: 03630 Accuracy: 0.953125\n",
      "Steps: 03640 Accuracy: 0.972656\n",
      "Steps: 03650 Accuracy: 0.976562\n",
      "Steps: 03660 Accuracy: 0.957031\n",
      "Steps: 03670 Accuracy: 0.933594\n",
      "Steps: 03680 Accuracy: 0.929688\n",
      "Steps: 03690 Accuracy: 0.953125\n",
      "Steps: 03700 Accuracy: 0.968750\n",
      "Steps: 03710 Accuracy: 0.953125\n",
      "Steps: 03720 Accuracy: 0.945312\n",
      "Steps: 03730 Accuracy: 0.957031\n",
      "Steps: 03740 Accuracy: 0.972656\n",
      "\n",
      "Train Accuracy: 0.950933 Loss: 2897482.006747\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.891776 \n",
      "\n",
      "Epoch: 33\n",
      "Steps: 03750 Accuracy: 0.968750\n",
      "Steps: 03760 Accuracy: 0.968750\n",
      "Steps: 03770 Accuracy: 0.941406\n",
      "Steps: 03780 Accuracy: 0.957031\n",
      "Steps: 03790 Accuracy: 0.933594\n",
      "Steps: 03800 Accuracy: 0.957031\n",
      "Steps: 03810 Accuracy: 0.968750\n",
      "Steps: 03820 Accuracy: 0.960938\n",
      "Steps: 03830 Accuracy: 0.957031\n",
      "Steps: 03840 Accuracy: 0.941406\n",
      "Steps: 03850 Accuracy: 0.964844\n",
      "Steps: 03860 Accuracy: 0.949219\n",
      "\n",
      "Train Accuracy: 0.952741 Loss: 2703153.179736\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.891240 \n",
      "\n",
      "Epoch: 34\n",
      "Steps: 03870 Accuracy: 0.957031\n",
      "Steps: 03880 Accuracy: 0.957031\n",
      "Steps: 03890 Accuracy: 0.968750\n",
      "Steps: 03900 Accuracy: 0.941406\n",
      "Steps: 03910 Accuracy: 0.929688\n",
      "Steps: 03920 Accuracy: 0.953125\n",
      "Steps: 03930 Accuracy: 0.949219\n",
      "Steps: 03940 Accuracy: 0.960938\n",
      "Steps: 03950 Accuracy: 0.933594\n",
      "Steps: 03960 Accuracy: 0.941406\n",
      "Steps: 03970 Accuracy: 0.929688\n",
      "\n",
      "Train Accuracy: 0.955387 Loss: 2539468.116811\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.893115 \n",
      "\n",
      "Epoch: 35\n",
      "Steps: 03980 Accuracy: 0.976562\n",
      "Steps: 03990 Accuracy: 0.945312\n",
      "Steps: 04000 Accuracy: 0.964844\n",
      "Steps: 04010 Accuracy: 0.937500\n",
      "Steps: 04020 Accuracy: 0.953125\n",
      "Steps: 04030 Accuracy: 0.968750\n",
      "Steps: 04040 Accuracy: 0.972656\n",
      "Steps: 04050 Accuracy: 0.968750\n",
      "Steps: 04060 Accuracy: 0.937500\n",
      "Steps: 04070 Accuracy: 0.945312\n",
      "Steps: 04080 Accuracy: 0.945312\n",
      "Steps: 04090 Accuracy: 0.945312\n",
      "\n",
      "Train Accuracy: 0.956794 Loss: 2403502.118726\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.894991 \n",
      "\n",
      "Epoch: 36\n",
      "Steps: 04100 Accuracy: 0.964844\n",
      "Steps: 04110 Accuracy: 0.972656\n",
      "Steps: 04120 Accuracy: 0.914062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 04130 Accuracy: 0.949219\n",
      "Steps: 04140 Accuracy: 0.945312\n",
      "Steps: 04150 Accuracy: 0.988281\n",
      "Steps: 04160 Accuracy: 0.941406\n",
      "Steps: 04170 Accuracy: 0.960938\n",
      "Steps: 04180 Accuracy: 0.949219\n",
      "Steps: 04190 Accuracy: 0.976562\n",
      "Steps: 04200 Accuracy: 0.964844\n",
      "Steps: 04210 Accuracy: 0.957031\n",
      "\n",
      "Train Accuracy: 0.958469 Loss: 2266098.496771\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.892848 \n",
      "\n",
      "Epoch: 37\n",
      "Steps: 04220 Accuracy: 0.937500\n",
      "Steps: 04230 Accuracy: 0.949219\n",
      "Steps: 04240 Accuracy: 0.972656\n",
      "Steps: 04250 Accuracy: 0.964844\n",
      "Steps: 04260 Accuracy: 0.968750\n",
      "Steps: 04270 Accuracy: 0.949219\n",
      "Steps: 04280 Accuracy: 0.945312\n",
      "Steps: 04290 Accuracy: 0.980469\n",
      "Steps: 04300 Accuracy: 0.984375\n",
      "Steps: 04310 Accuracy: 0.960938\n",
      "Steps: 04320 Accuracy: 0.914062\n",
      "\n",
      "Train Accuracy: 0.961316 Loss: 2144089.130429\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.890437 \n",
      "\n",
      "Epoch: 38\n",
      "Steps: 04330 Accuracy: 0.953125\n",
      "Steps: 04340 Accuracy: 0.968750\n",
      "Steps: 04350 Accuracy: 0.945312\n",
      "Steps: 04360 Accuracy: 0.972656\n",
      "Steps: 04370 Accuracy: 0.980469\n",
      "Steps: 04380 Accuracy: 0.960938\n",
      "Steps: 04390 Accuracy: 0.964844\n",
      "Steps: 04400 Accuracy: 0.972656\n",
      "Steps: 04410 Accuracy: 0.968750\n",
      "Steps: 04420 Accuracy: 0.957031\n",
      "Steps: 04430 Accuracy: 0.992188\n",
      "Steps: 04440 Accuracy: 0.976562\n",
      "\n",
      "Train Accuracy: 0.962387 Loss: 2002446.272008\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.892044 \n",
      "\n",
      "Epoch: 39\n",
      "Steps: 04450 Accuracy: 0.972656\n",
      "Steps: 04460 Accuracy: 0.976562\n",
      "Steps: 04470 Accuracy: 0.960938\n",
      "Steps: 04480 Accuracy: 0.972656\n",
      "Steps: 04490 Accuracy: 0.949219\n",
      "Steps: 04500 Accuracy: 0.960938\n",
      "Steps: 04510 Accuracy: 0.941406\n",
      "Steps: 04520 Accuracy: 0.933594\n",
      "Steps: 04530 Accuracy: 0.960938\n",
      "Steps: 04540 Accuracy: 0.972656\n",
      "Steps: 04550 Accuracy: 0.953125\n",
      "Steps: 04560 Accuracy: 0.945312\n",
      "\n",
      "Train Accuracy: 0.964163 Loss: 1929192.286776\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.896062 \n",
      "\n",
      "Epoch: 40\n",
      "Steps: 04570 Accuracy: 0.968750\n",
      "Steps: 04580 Accuracy: 0.964844\n",
      "Steps: 04590 Accuracy: 0.976562\n",
      "Steps: 04600 Accuracy: 0.960938\n",
      "Steps: 04610 Accuracy: 0.964844\n",
      "Steps: 04620 Accuracy: 0.964844\n",
      "Steps: 04630 Accuracy: 0.953125\n",
      "Steps: 04640 Accuracy: 0.960938\n",
      "Steps: 04650 Accuracy: 0.980469\n",
      "Steps: 04660 Accuracy: 0.945312\n",
      "Steps: 04670 Accuracy: 0.976562\n",
      "Steps: 04680 Accuracy: 0.975155\n",
      "\n",
      "Train Accuracy: 0.965335 Loss: 1869249.257724\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.897402 \n",
      "\n",
      "Epoch: 41\n",
      "Steps: 04690 Accuracy: 0.972656\n",
      "Steps: 04700 Accuracy: 0.960938\n",
      "Steps: 04710 Accuracy: 0.972656\n",
      "Steps: 04720 Accuracy: 0.980469\n",
      "Steps: 04730 Accuracy: 0.972656\n",
      "Steps: 04740 Accuracy: 0.976562\n",
      "Steps: 04750 Accuracy: 0.980469\n",
      "Steps: 04760 Accuracy: 0.972656\n",
      "Steps: 04770 Accuracy: 0.957031\n",
      "Steps: 04780 Accuracy: 0.937500\n",
      "Steps: 04790 Accuracy: 0.953125\n",
      "\n",
      "Train Accuracy: 0.965134 Loss: 1795084.661182\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.894187 \n",
      "\n",
      "Epoch: 42\n",
      "Steps: 04800 Accuracy: 0.957031\n",
      "Steps: 04810 Accuracy: 0.964844\n",
      "Steps: 04820 Accuracy: 0.972656\n",
      "Steps: 04830 Accuracy: 0.972656\n",
      "Steps: 04840 Accuracy: 0.960938\n",
      "Steps: 04850 Accuracy: 0.941406\n",
      "Steps: 04860 Accuracy: 0.988281\n",
      "Steps: 04870 Accuracy: 0.957031\n",
      "Steps: 04880 Accuracy: 0.957031\n",
      "Steps: 04890 Accuracy: 0.976562\n",
      "Steps: 04900 Accuracy: 0.968750\n",
      "Steps: 04910 Accuracy: 0.957031\n",
      "\n",
      "Train Accuracy: 0.966340 Loss: 1759947.923911\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.898205 \n",
      "\n",
      "Epoch: 43\n",
      "Steps: 04920 Accuracy: 0.960938\n",
      "Steps: 04930 Accuracy: 0.972656\n",
      "Steps: 04940 Accuracy: 0.968750\n",
      "Steps: 04950 Accuracy: 0.972656\n",
      "Steps: 04960 Accuracy: 0.964844\n",
      "Steps: 04970 Accuracy: 0.968750\n",
      "Steps: 04980 Accuracy: 0.964844\n",
      "Steps: 04990 Accuracy: 0.968750\n",
      "Steps: 05000 Accuracy: 0.972656\n",
      "Steps: 05010 Accuracy: 0.964844\n",
      "Steps: 05020 Accuracy: 0.972656\n",
      "Steps: 05030 Accuracy: 0.984375\n",
      "\n",
      "Train Accuracy: 0.968349 Loss: 1647260.560807\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.896598 \n",
      "\n",
      "Epoch: 44\n",
      "Steps: 05040 Accuracy: 0.957031\n",
      "Steps: 05050 Accuracy: 0.968750\n",
      "Steps: 05060 Accuracy: 0.972656\n",
      "Steps: 05070 Accuracy: 0.945312\n",
      "Steps: 05080 Accuracy: 0.976562\n",
      "Steps: 05090 Accuracy: 0.968750\n",
      "Steps: 05100 Accuracy: 0.957031\n",
      "Steps: 05110 Accuracy: 0.976562\n",
      "Steps: 05120 Accuracy: 0.980469\n",
      "Steps: 05130 Accuracy: 0.960938\n",
      "Steps: 05140 Accuracy: 0.953125\n",
      "\n",
      "Train Accuracy: 0.969488 Loss: 1584859.981164\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.893651 \n",
      "\n",
      "Epoch: 45\n",
      "Steps: 05150 Accuracy: 0.980469\n",
      "Steps: 05160 Accuracy: 0.968750\n",
      "Steps: 05170 Accuracy: 0.972656\n",
      "Steps: 05180 Accuracy: 0.957031\n",
      "Steps: 05190 Accuracy: 0.960938\n",
      "Steps: 05200 Accuracy: 0.964844\n",
      "Steps: 05210 Accuracy: 0.964844\n",
      "Steps: 05220 Accuracy: 0.984375\n",
      "Steps: 05230 Accuracy: 0.976562\n",
      "Steps: 05240 Accuracy: 0.953125\n",
      "Steps: 05250 Accuracy: 0.953125\n",
      "Steps: 05260 Accuracy: 0.976562\n",
      "\n",
      "Train Accuracy: 0.969588 Loss: 1537226.561633\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.894991 \n",
      "\n",
      "Epoch: 46\n",
      "Steps: 05270 Accuracy: 0.964844\n",
      "Steps: 05280 Accuracy: 0.964844\n",
      "Steps: 05290 Accuracy: 0.953125\n",
      "Steps: 05300 Accuracy: 0.968750\n",
      "Steps: 05310 Accuracy: 0.957031\n",
      "Steps: 05320 Accuracy: 0.972656\n",
      "Steps: 05330 Accuracy: 0.980469\n",
      "Steps: 05340 Accuracy: 0.960938\n",
      "Steps: 05350 Accuracy: 0.980469\n",
      "Steps: 05360 Accuracy: 0.988281\n",
      "Steps: 05370 Accuracy: 0.964844\n",
      "Steps: 05380 Accuracy: 0.980469\n",
      "\n",
      "Train Accuracy: 0.970995 Loss: 1477876.467800\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.894723 \n",
      "\n",
      "Epoch: 47\n",
      "Steps: 05390 Accuracy: 0.964844\n",
      "Steps: 05400 Accuracy: 0.968750\n",
      "Steps: 05410 Accuracy: 0.949219\n",
      "Steps: 05420 Accuracy: 0.968750\n",
      "Steps: 05430 Accuracy: 0.968750\n",
      "Steps: 05440 Accuracy: 0.953125\n",
      "Steps: 05450 Accuracy: 0.972656\n",
      "Steps: 05460 Accuracy: 0.949219\n",
      "Steps: 05470 Accuracy: 0.976562\n",
      "Steps: 05480 Accuracy: 0.984375\n",
      "Steps: 05490 Accuracy: 0.949219\n",
      "\n",
      "Train Accuracy: 0.971096 Loss: 1441883.563125\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.896598 \n",
      "\n",
      "Epoch: 48\n",
      "Steps: 05500 Accuracy: 0.960938\n",
      "Steps: 05510 Accuracy: 0.968750\n",
      "Steps: 05520 Accuracy: 0.984375\n",
      "Steps: 05530 Accuracy: 0.976562\n",
      "Steps: 05540 Accuracy: 0.960938\n",
      "Steps: 05550 Accuracy: 0.972656\n",
      "Steps: 05560 Accuracy: 0.960938\n",
      "Steps: 05570 Accuracy: 0.949219\n",
      "Steps: 05580 Accuracy: 0.980469\n",
      "Steps: 05590 Accuracy: 0.984375\n",
      "Steps: 05600 Accuracy: 0.976562\n",
      "Steps: 05610 Accuracy: 0.976562\n",
      "\n",
      "Train Accuracy: 0.971631 Loss: 1377344.662315\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.896866 \n",
      "\n",
      "Epoch: 49\n",
      "Steps: 05620 Accuracy: 0.949219\n",
      "Steps: 05630 Accuracy: 0.984375\n",
      "Steps: 05640 Accuracy: 0.976562\n",
      "Steps: 05650 Accuracy: 0.980469\n",
      "Steps: 05660 Accuracy: 0.980469\n",
      "Steps: 05670 Accuracy: 0.980469\n",
      "Steps: 05680 Accuracy: 0.972656\n",
      "Steps: 05690 Accuracy: 0.980469\n",
      "Steps: 05700 Accuracy: 0.976562\n",
      "Steps: 05710 Accuracy: 0.964844\n",
      "Steps: 05720 Accuracy: 0.976562\n",
      "Steps: 05730 Accuracy: 0.980469\n",
      "\n",
      "Train Accuracy: 0.973507 Loss: 1303197.659091\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.893651 \n",
      "\n",
      "Epoch: 50\n",
      "Steps: 05740 Accuracy: 0.980469\n",
      "Steps: 05750 Accuracy: 0.960938\n",
      "Steps: 05760 Accuracy: 0.992188\n",
      "Steps: 05770 Accuracy: 0.984375\n",
      "Steps: 05780 Accuracy: 0.984375\n",
      "Steps: 05790 Accuracy: 0.976562\n",
      "Steps: 05800 Accuracy: 0.964844\n",
      "Steps: 05810 Accuracy: 0.984375\n",
      "Steps: 05820 Accuracy: 0.960938\n",
      "Steps: 05830 Accuracy: 0.972656\n",
      "Steps: 05840 Accuracy: 0.988281\n",
      "Steps: 05850 Accuracy: 0.981366\n",
      "\n",
      "Train Accuracy: 0.974043 Loss: 1239882.349175\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.894723 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model end evaluate on our validation set\n",
    "\n",
    "ic_loss_list = []\n",
    "ic_train_acc_list = []\n",
    "ic_val_acc_list = []\n",
    "\n",
    "ic_steps = 0\n",
    "for epoch in range(1, options['epochs'] + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    # variables to keep track of our average loss and accuracy\n",
    "    ic_loss_avg = 0\n",
    "    train_acc_avg = 0\n",
    "\n",
    "    # run batched training\n",
    "    for batch_idxs in get_batch_idxs(len(X_train), options['batch_size']):\n",
    "        \n",
    "        _, accur, curr_cls_loss = sess.run(\n",
    "            [cls_train_op, cls_acc, cls_loss], feed_dict= {\n",
    "                im: X_train[batch_idxs], \n",
    "                y: y_train[batch_idxs]\n",
    "            })\n",
    "        \n",
    "        ic_steps += 1\n",
    "        \n",
    "        # update average loss and accuracy\n",
    "        ic_loss_avg += curr_cls_loss * len(batch_idxs) / float(len(X_train))\n",
    "        train_acc_avg += accur * len(batch_idxs) / float(len(X_train))\n",
    "\n",
    "        if ic_steps % 10 == 0:\n",
    "            print(\"Steps: %05d Accuracy: %f\" % (\n",
    "                ic_steps, accur))\n",
    "\n",
    "    print()\n",
    "    print(\"Train Accuracy: %f Loss: %f\\n\" % (\n",
    "            train_acc_avg, \n",
    "            ic_loss_avg\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ic_loss_list.append(ic_loss_avg)\n",
    "    ic_train_acc_list.append(train_acc_avg)\n",
    "\n",
    "    # variables to keep track of our average val loss and accuracy\n",
    "    val_acc_avg = 0\n",
    "    \n",
    "    # get validation accuracy in batches\n",
    "    for batch_idxs in get_batch_idxs(len(X_val), options['batch_size']):\n",
    "        [curr_val_accur] = sess.run(\n",
    "            [cls_acc], feed_dict={ \n",
    "                im: X_val[batch_idxs], \n",
    "                y: y_val[batch_idxs]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        val_acc_avg += curr_val_accur * len(batch_idxs) / float(len(X_val))\n",
    "\n",
    "    print(\"\\nValidation Accuracy: %f \\n\" % (val_acc_avg))\n",
    "    \n",
    "    ic_val_acc_list.append(val_acc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
